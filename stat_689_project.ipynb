{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stat-689-project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/delimited0/stat-689-project/blob/master/stat_689_project.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "bQfZQv59t9gC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Abstract:\n",
        "\n",
        "Hamiltonian Monte Carlo (HMC) is a MCMC method used to sample from intractable priors. Rate of convergence to the stationary distribution and mixing is compared to Gibbs sampling and Metropolis-Hastings algorithms. Tuning parameters of HMC are explored."
      ]
    },
    {
      "metadata": {
        "id": "38jLnMI5fbed",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction:\n",
        "\n",
        "Hamiltonian Monte Carlo (HMC, Hybrid Monte Carlo) is an MCMC method for sampling from an intractable posterior. HMC improves upon the Metropolis-Hastings algorithm by reducing the autocorrelation of the Markov chain generated. As such, convergence is attained more quickly, reducing necessary computational time.\n",
        "\n",
        "The Hamiltonian of a physical system is the following:\n",
        "\n",
        "\n",
        "\\begin{aligned}\n",
        "H(q, p) = K + U\n",
        "\\end{aligned}\n",
        "\n",
        "Where K is the kinetic energy and U is the potential energy of the system, written in terms of momentum(p) and position (q). The following two first order differential equations are satisfied:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\frac{dp}{dt} = - \\frac{\\partial H}{\\partial q} \\quad\\quad \\frac{dq}{dt} = \\frac{\\partial H}{\\partial p}\n",
        "\\end{aligned}\n",
        "\n",
        "Now consider the following joint density\n",
        "\n",
        "\\begin{aligned}\n",
        "h(p,q) = k*\\exp(-H)\n",
        "\\end{aligned}\n",
        "\n",
        "Where k is a normalization constant. It can easily be shown that p and q are independent by noting that the kinetic energy depends only on p, while the potential energy depends only on q.\n",
        "\n",
        "\n",
        "\\begin{aligned}\n",
        "H(q,p) &= K(p,q) + U(q)\n",
        "\\\\\n",
        "H(q,p) &= K(p) + U(q)\n",
        "\\\\\n",
        "\\pi(q, p) &= e^{-K(p) - U(q)} \n",
        "\\\\\n",
        "&= e^{-K(p)}e^{-U(q)}\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "The obvious choice for potential energy is performing a negative log transform of the density f that we wish to sample from.\n",
        "\\begin{aligned}\n",
        "U = -\\log(f)\n",
        "\\end{aligned}\n",
        "\n",
        "As p and q are independent variables the marginal density of h is ,\n",
        "\n",
        "\\begin{aligned}\n",
        "h(q) = \\exp(-U)\n",
        "= \\exp(-(-log(f))\n",
        "= f\n",
        "\\end{aligned}\n",
        "\n",
        "Note that intractable posteriors are simply probability densities multiplied by an unknown normalization constant. Therefore the log transform has no domain issues and the potential energy approaches infinity as the density approaches zero. If the density is exactly 0 at some value, the potential energy is considered infinite).\n",
        "\n",
        "Our choice of kinetic energy is the translational, non-relativistic kinetic energy,\n",
        "\n",
        "\\begin{aligned}\n",
        "K = 0.5 m * v^2\n",
        "\\end{aligned}\n",
        "\n",
        "Other choices of kinetic energy may be used, for instance we could add a rotational component to the kinetic energy as well (so the physical system equivalent would be analogous to a ball rolling up and down hills as opposed to a frictionless mass sliding. However, the additional term is unnecessary and makes calculations more complicated for no gain.\n",
        "\n",
        "Now, given a starting location and momentum, it is possible to calculate the time evolution of the system (using the two first order differential equations). The system is deterministic, and after some time, we can calculate the new q and the new p. First we update momentum using half of a time step. Then we alternate updating position and momentum using full time steps. On the final update of momentum, we update with a final half time step. This guarantees that updating $q(t)$ to $q(t+1)$, we will use $p(t+0.5)$, a rough estimate of the average of p between times t and t+1.  This is known as the leapfrog algorithm.\n",
        "\n",
        "According to Hamilton’s equations, the change of momentum depends on position, and the change of position depends on momentum. If we iterate this system using a certain number of time steps, an error is induced and the total energy may increase or decrease.  Using the leapfrog algorithm helps reduce the magnitude of this error, but will not entirely remove it.  If we make some minor adjustments (soon we will demonstrate that nothing actually has to change), we may make a Metropolis-Hastings step and accept or reject the new position as another sample from the intractable prior.\n",
        "\n",
        "The Metropolis-Hastings acceptance probability is\n",
        "\\begin{aligned}\n",
        "A(x'|x) = \\min\\{1,\\frac{P(x')}{P(x)}\\frac{g(x'|x)}{g(x|x')}\\}\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "However, iterating through Hamilton's equations is a deterministic process. $g(x'|x) = 1$ always, but $g(x|x')$ generally is 0.  If at the end of the step we negate the momentum, this will force both $g(x'|x) = 1$, and $g(x|x') = 1$.  Negating momentum at the end of the time transition ensures the physical system is time reversible.  In practice, we may leave our momentum untouched, as kinetic energy is based on the square of momentum.\n",
        "\n",
        "Thus,\n",
        "\n",
        "\\begin{aligned}\n",
        "A(x'|x) &= \\min\\{1,\\frac{P(x')}{P(x)} \\frac{1}{1} \\}\\\\\n",
        "&= \\min\\{1, \\frac{P(x')}{P(x)}\\}\\\\\n",
        "&= \\min\\{1,\\frac{k\\exp(-H(x'))}{k\\exp(-H(x))}\\}\\\\\n",
        "&= \\min\\{1,\\exp(-H(x') + H(x))\\}\n",
        "\\end{aligned}\n",
        "\n",
        "To convince yourself that changing the sign of momentum at the end of the time transition guarantees time reversibility, imagine throwing a perfectly elastic ball from a height such that the ball hits the floor at the end of the time transition.  After a second time transition, the ball is where it was originally thrown from, but travelling upwards with the initial speed of the throw.  Negating the momentum again brings us back to the starting state."
      ]
    },
    {
      "metadata": {
        "id": "_RiuvVppj-mF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Methods:\n",
        "\n",
        "So our goal is to try to implement different MCMC methods on different data, and see how fast HMC, Gibbs, and MH (Metropolis-Hastings) can get samples, and the quality of the samples they obtain.\n",
        "\n",
        "To examine the “quality of the sample” that a MCMC algorithm produces, we look to examine their effective sample size. The effective sample size indicates how many independent draws we’ve made, using our MCMC algorithm, which uses dependent draws. To first make sure our algorithm works soundly, we examine how well our chains do on simulated data. \n",
        "\n",
        "We have coded out samplers to be very general. For Gibbs sampling, we pass a list of initial parameters, the number of iterations we want to run it, and functions that can draw from conditional distributions, and then our Gibbs sampler will just update the parameters using the conditionals. For MH our sampler is passed intial parameters, the number of iterations, and functions that can evaluate the likelihood and the prior as well as a function that can draw from the proposal distribution and evaluate the proposal distribution. We then use these functions to update our parameters for MH. HMC takes a function that is porportional to the negative log of the posterior, a function that evaluates the derivative at the negative log of the posterior, a list of inital parameters, the number of iterations, and tuning parameters and then updates. \n",
        "\n",
        "We first do this on the conjugate normal-normal model. The model is conjugate so we know what the posterior mean and variance are, and we can check our chains to make sure they’re all behaving appropriately.\n"
      ]
    },
    {
      "metadata": {
        "id": "9h7H_COQkNIo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normal-normal model \n",
        "### [code](https://github.com/delimited0/stat-689-project/blob/master/models/normal-normal.ipynb)\n",
        "\n",
        "\\begin{aligned}\n",
        "x_1 \\ldots x_n|\\mu &\\sim N(\\mu, 1) \n",
        "\\\\\n",
        "\\mu &\\propto 1\n",
        "\\end{aligned}\n",
        "\n",
        "Our effective size for the normal normal model when we used HMC was around 824 samples, while our effective size when we used MH was around 330 sample sizes.\n",
        "\n",
        "The amount of time it took to run the HMC algorithm on average is 1381.025 milliseconds.  The MH algorithm took on average around 306.7682 milliseconds\n",
        "\n",
        "We're not surprised HMC had higher effective size and took longer, but in this case, it feels like MH is more efficient at getting effective samples. If we had run MH for 3 times longer, we would have a higher effective size then HMC while taking less time to run.  "
      ]
    },
    {
      "metadata": {
        "id": "SwGZ9cIgkQq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multivariate Normal\n",
        "### [code](https://github.com/delimited0/stat-689-project/blob/master/models/21dimmvn.r)\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mu \\in \\mathbb{R}^{21}, \n",
        "x_1 \\ldots x_n|\\mu &\\sim N(\\mu, 0.5 * I_{21}) \n",
        "\\\\\n",
        "\\end{aligned}\n",
        "\n",
        "The true $\\mu$ is\n",
        "\n",
        "\\begin{aligned}\n",
        "\\mu = [10, 9, 8, ..., -8, -9, -10]^T\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "We wanted to see if moving into higher dimensions would change the effectiveness of any of the samplers. So to do this we simulate our data so that they are drawn from a 21 dimensional multivariate normal distribution.  We then try to infer the parameters of the original simulated data. We have 20 different means changing constantly so we're monitoring 20 chains in some sense.\n",
        "\n",
        "Our effective sample sizes are as follows for the different samplers:\n",
        "\n",
        "|Method     | Effective Sample Size$^1$|\n",
        "|:----------|---------------------:|\n",
        "|MH         |                   115|\n",
        "|Gibbs         |                   10013|\n",
        "|HMC        |                 93305|\n",
        "\n",
        "\n",
        "\n",
        "The median time it took to draw from MH 10000 times is 263 milliseconds.  The median time it took to draw 10000 times from HMC is 1.48 seconds.  Gibbs sampling 10000 samples had a median time of 334 milliseconds.\n",
        "\n",
        "We only have 10000 samples, but HMC seems to give an effective sample size that is much greater than that. We suspect this is because our variables are dependent but have negative autocorrelation. As a result, sampling the dependent variable multiple times is better than sampling an independent variable the same number of times. \n",
        "\n",
        "\n",
        "Our effective sample sizes normalized with respect to time are as follows for the different samplers:\n",
        "\n",
        "|Method     | Effective Sample Size / Time (s)|\n",
        "|:----------|---------------------:|\n",
        "|MH         |                   436|\n",
        "|Gibbs         |                   29954|\n",
        "|HMC        |                 63006|\n",
        "\n",
        "HMC is the clear best choice for this model."
      ]
    },
    {
      "metadata": {
        "id": "wkIVFfXTkUth",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normal-Inverse Gamma\n",
        "### [code](https://github.com/delimited0/stat-689-project/blob/master/models/nig.Rmd)\n",
        "\n",
        "We examine a Normal - inverse gamma model next. The model is:\n",
        "\n",
        "\\begin{aligned}\n",
        "x_1,\\ldots,x_n|\\mu,\\sigma^2 &\\sim N(\\mu, \\sigma^2) \\\\\n",
        "\\pi(\\mu, \\sigma^2) &\\propto \\frac{1}{\\sigma^2} \n",
        "\\end{aligned}\n",
        "\n",
        "It has fewer parameters then our multivariate normal example, but our concern was that our original normal-normal model was too simple, and wanted to see if changing how we draw our variances affects which sampler is optimal.\n",
        "\n",
        "The median effective sample size per second from 20 simulations are as follows:\n",
        "\n",
        "|Method     |       mu| sigmasq|\n",
        "|:----------|--------:|-------:|\n",
        "|MH         |  1018.84| 1224.16|\n",
        "|Gibbs      | 10360.68| 8692.04|\n",
        "|HMC        |  1171.29|  798.83|\n",
        "\n",
        "\n",
        "So the Gibbs is more efficient then either MH or HMC in this case. Gibbs sampling is suited to this problem because of the conjugate priors and few number of parameters to estimate."
      ]
    },
    {
      "metadata": {
        "id": "4GXwrinFkZLI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes: Digit Data\n",
        "### [code](https://github.com/delimited0/stat-689-project/blob/master/models/naive_bayes.Rmd)\n",
        "\n",
        "So after observing how Gibbs, MH, and HMC interact with simulated data, we thought it was a good idea to try this with real data. \n",
        "\n",
        "Using the `sklearn` 8 x 8  MNIST dataset, we obtain very small pictures (they are of size 8 x 8 pixels) of images that are labeled as an integer from 0 to 9. For each image, we observe “how dark” a pixel is, which is a number from 0 to 16, with 0 being white and 16 being black. There are 1797 total images. Here is an example of a zero:\n",
        "\n",
        "![digit](images/digit.png)\n",
        "\n",
        "We decide to fit a naive-bayes model. Each image can be represented equivalently as an 8 x 8 matrix that takes some values from 0 to 16. Each image is an integer from 0 to 9, so we imagine that for each image, there are 10 different multivariate normal distributions that they could be like, one for each integer. We don’t know the mean vector or covariance matrix, so we use our MCMC methods to try to get an idea for what they are.\n",
        "\n",
        "In trying to compute the variance matrix, we realize that the number of parameters might be too large to compute, and in an attempt to speed up our code, we decide to change our model so that we no longer assume a general covariance matrix, but rather assume that the variance of pixels for an image are independent and identically distributed. So all the pixels in a particular class or that are a particular integer should have common variance and be independent, and each integer has a different variance. We realize this may not be entirely true, but doing this allows us to go from observing ten matrices that are of size 64 x 64 to observing only 10 variances overall, which speeds up computation. \n",
        "\n",
        "Let $x_n \\in \\mathbb{R}^{64}$ be the $n$th image, $y_n \\in \\{0,1,\\ldots,9\\}$ its label. The model is:\n",
        "\n",
        "\\begin{aligned}\n",
        "x_n|y_n = k,\\mu,\\Sigma &\\sim N(\\mu_{k}, \\sigma_{k}^2I_{8}) \n",
        "\\\\\n",
        "\\mu_k &\\sim N(0, I_{8}) \n",
        "\\\\\n",
        "\\sigma_k^2 &\\sim IG(a/2, b/2)\n",
        "\\\\\n",
        "n = 1,\\ldots,N &\\quad k = 0,\\ldots,9\n",
        "\\end{aligned}\n",
        "\n",
        "We give a training dataset, and have the computer get estimates for the means and variances for each class by using Gibbs, MH, and HMC. We then use our estimates with a testing set. We compute the log likelihood of obtaining any class using our estimates, then we have the computer guess the digit its tested on as the one that has the highest likelihood. \n",
        "\n",
        "This is the posterior mean for the zero class, fit through HMC:\n",
        "\n",
        "![digit](images/hmc_nb_post_mean.png)\n",
        "\n",
        "One way to test how well our samplers do, is to examine how accurate we were under different MCMC methods. \n",
        "\n",
        "We run each method we draw one thousand samples and examine accuracy and run times to get an idea how each MCMC method did at producing estimates. We noticed while running HMC for the previous simulations that it could be very difficult to tune to get good mixing, but when it was that it gave great results. Tuning HMC felt much more tedious and difficult then tuning MH, so we used the `R` package [`greta`](https://greta-dev.github.io/greta/) to perform HMC.\n",
        "\n",
        "The accuracy for each sampler was:\n",
        "\n",
        "| Method        | Accuracy|\n",
        "|:--------------|--------:|\n",
        "|MH             |     0.86|\n",
        "|Gibbs          |     0.89|\n",
        "|HMC            |     0.89|\n",
        "\n",
        "\n",
        "The run time for each sampler was: \n",
        "\n",
        "| Method       | Seconds|\n",
        "|:-------------|-------:|\n",
        "|MH            |    7.31|\n",
        "|Gibbs         |    5.96|\n",
        "|HMC           |  143.71|\n",
        "\n",
        "So the Gibbs sampler and the HMC sampler have similar accuracy, but the HMC sampler take much longer to run. The MH sampler is very fast, but is less accurate then both the HMC and the Gibbs sampler. \n",
        "\n",
        "We acknowledge that the HMC took a very long time to run, but we’re using `greta` to tune and to create graphs\n",
        "\n",
        "We suspect this is because our model is very simple and the conditionals are easy to sample from. If the conditionals are more complicated or are harder to draw from we imagine the the time it takes to run Gibbs to increase."
      ]
    },
    {
      "metadata": {
        "id": "NnHggh2QeBOf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Concluding Remarks\n",
        "\n",
        "The benefits of using HMC instead of MH in efficiently gaining effective size over time is more visible in higher dimensions than lower ones. For our Normal-Normal base model, MH was more efficient in gaining effective size over time then HMC, but for our 21 dimension multivariate normal model, HMC was much better. \n",
        "\n",
        "Gibbs appears to do very well, but we believe this is due to our choice of priors which allowed for easy drawing from full conditionals. If the conditional distributions were a little harder to sample from, or if we only knew what the conditional distributions were proportional to, Gibbs would be more difficult or impossible to implement.\n",
        "\n",
        "Hand tuning HMC is a long process which relies on trial and error.  In the digit data, rather than hand tune HMC, we found it better to just use a package (`greta`) that automatically tunes the sampler for us. We hand tuned HMC for the all of the simulated cases. Had we given the time for just a hand-tuned run, we'd expect the time it takes for the HMC sampler to run to take far less time. There's a cost involved for appropriately tuning both HMC and MH that our report has not examined.  Further work will look into automatic tuning of HMC.\n"
      ]
    },
    {
      "metadata": {
        "id": "p6HGwwdbxSco",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}