---
title: "R Notebook"
output: html_notebook
---

```{r message = FALSE}
library(tidyverse)
library(rstan)
```

```{r}
mnist <- read_csv("mnist.csv")
y <- mnist$label
X <- as.matrix(mnist[, -1])
K <- length(unique(y))

train_id <- sample(1:nrow(X), nrow(X) / 100, FALSE)
test_id <- (1:nrow(X))[-train_id]

X_train <- X[train_id, ]
y_train <- y[train_id]
X_test <- X[test_id, ]
y_test <- y[test_id]

mnist_stan_dat <- list(K = K, N = nrow(X_train), D = ncol(X_train), 
                       x = X_train, y = y_train + 1,
                       alpha = rep(.1, K), a = 2, b = 2)

image(matrix(X[1603, ], nrow = 28))
y[1603]
```

## sampling here

```{r}
source("gibbs.R")
source("utility.R")
source("naive_bayes.R")
source("naive_bayes_mh.R")
set.seed(1)


nb_param_init <- list(pi = rep(.1, K), 
                      mus = matrix(0, ncol(X_train), K),
                      sigmas = rep(1, K))

nb_gibbs <- gibbs(nb_param_init,
                  iters = 10,
                  nb_cond_pi(y_train, .1), 
                  nb_cond_mu(y_train, X_train), 
                  nb_cond_sigmas(y_train, X_train, 2, 2))

nb_mh <- nb_metropolis_hastings(nb_param_init, iters = 10, X_train, y_train, 
                                .1, 2, 2, pi_s = 100)

nb_hmc <- stan(file = "naive_bayes.stan", data = mnist_stan_dat, chains = 1,
               iter = 10)

nb_hmc <- nb_hmc_model(y_train, X_train, rep(.1, 10), 2, 2)
nb_hmc_draws <- greta::mcmc(nb_hmc, n_samples = 2, warmup = 1, verbose = TRUE)

image(matrix(colMeans(nb_mh$samples$mus[-101, , 6]), nrow = 28))
image(matrix(colMeans(nb_gibbs$samples$mus[900:1000, , 3]), nrow = 28))
```

```{r}
predict_nb(nb_gibbs$samples$mus[10, , ], nb_gibbs$samples$sigmas[10, ],
           X_test[1:10, ])
predict_nb(nb_mh$samples$mus[10, , ], nb_mh$samples$sigmas[10, ],
           X_test[1:10, ])

image(matrix(X_test[1, ], nrow = 28))
y_test[1:10]
```

